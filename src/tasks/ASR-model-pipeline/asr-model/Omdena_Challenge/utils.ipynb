{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOmLBMTX+mJ0i1C8aeBNJLi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"hCkwGDuGGbxw","executionInfo":{"status":"error","timestamp":1662858934725,"user_tz":300,"elapsed":226,"user":{"displayName":"Aren Wilson-Wright","userId":"05722690186564388330"}},"colab":{"base_uri":"https://localhost:8080/","height":353},"outputId":"b824b30c-1db1-4413-db12-57d26985570e"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-3bfeaca8e5f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytube\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mYouTube\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpydub\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAudioSegment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmake_chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytube'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["%%capture install_log\n","!apt-get install libsox-fmt-all libsox-dev sox\n","!pip install sox pytube pydub crepe transformers torchtext==0.12.0 pyannote.audio"]},{"cell_type":"code","source":["import os\n","import re\n","import sox\n","import librosa\n","import torch\n","import spacy\n","import crepe\n","import numpy as np\n","import pandas as pd\n","import datetime as dt\n","from pytube import YouTube\n","from pydub import AudioSegment\n","from pydub.utils import make_chunks\n","from numpy.linalg import norm\n","from IPython.display import Audio, display\n","from scipy.special import expit\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split, cross_val_score\n","from sklearn.metrics import mean_squared_error\n","from lightgbm.sklearn import LGBMRegressor\n","from hyperopt import tpe, hp, fmin, STATUS_OK, Trials "],"metadata":{"id":"GsAY4-g_YuBo","colab":{"base_uri":"https://localhost:8080/","height":390},"executionInfo":{"status":"error","timestamp":1662930156791,"user_tz":300,"elapsed":297,"user":{"displayName":"Aren Wilson-Wright","userId":"05722690186564388330"}},"outputId":"5b9c76cd-8238-4579-ce24-ae97b58a211d"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-c6f988db884a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sox'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["def make_directories(directories):\n","\n","  \"\"\"\n","  This function creates subdirectories from a user-provided list\n","  \"\"\"\n","\n","  ROOT = os.getcwd()\n","\n","  for directory in directories:\n","    os.makedirs(os.path.join(ROOT, directory), exist_ok=True)"],"metadata":{"id":"VkzL0lS3X2nw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def retrieve_audio(video_link, output_dir):\n","\n","  \"\"\"\n","  This function scrapes the audio from a YouTube video and saves it in the output directory\n","  \"\"\"\n","\n","  ROOT = os.getcwd()\n","  output = os.path.join(ROOT, output_dir)\n","  os.makedirs(output, exist_ok=True)\n","    \n","  try:\n","    video = YouTube(video_link)\n","\n","    audio = video.streams.filter(only_audio=True, file_extension='mp4').first()\n","\n","    audio.download(output_dir)\n","    \n","  except:\n","    print(\"Connection error\")"],"metadata":{"id":"NR5_t9E9HFKv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def chunk_audio(input_path, output_dir, chunk_length=30000, output_format='wav'):\n","\n","    \"\"\"\n","    This function divides an audio file into chunks of the given length (in msec)\n","    \"\"\"\n","    \n","    ROOT = os.getcwd()\n","    output = os.path.join(ROOT, output_dir)\n","    os.makedirs(output, exist_ok=True)\n","\n","    audio = AudioSegment.from_file(input_path)\n","    chunks = make_chunks(audio, chunk_length)\n","\n","    for i, chunk in enumerate(chunks):\n","        chunk_name = f\"{output}/chunk_{i}.wav\"\n","        chunk.export(chunk_name, format=output_format)"],"metadata":{"id":"GrlD4qlsGhvk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def convert_to_wav(input_path, output_dir):\n","\n","    \"\"\"\n","    This function converts and audio file into the .wav format and saves it in the output directory\n","    \"\"\"\n","    \n","    ROOT = os.getcwd()\n","    output = os.path.join(ROOT, output_dir)\n","    os.makedirs(output, exist_ok=True)\n","\n","    file_name = re.split(\"/|\\.\", input_path)[-2]\n","\n","    audio = AudioSegment.from_file(input_path)\n","    audio.export(f\"{output_dir}/{file_name}.wav\", format='wav')"],"metadata":{"id":"VAPxQ3OaXXWO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def diarization_profiler(file, pipeline):\n","\n","  \"\"\"\n","  This function diarizes the provided audio file and calculates some summary statistics about each turn, which are stored in the dataframes c_turns and turn_profile\n","  \"\"\"\n","\n","  diarization = pipeline(file['audio'])\n","\n","  turns = pd.DataFrame(columns=['speaker', 'start_time', 'end_time'])\n","\n","  for turn, _, speaker in diarization.itertracks(yield_label=True):\n","        turns = turns.append({'speaker' : speaker, 'start_time' : turn.start, 'end_time' : turn.end}, ignore_index=True)\n","\n","  speakers = {speaker : i for i, speaker in enumerate(sorted(turns.speaker.unique()))}\n","\n","  turns['speaker'].replace(speakers.keys(), speakers.values(), inplace=True)\n","\n","  ignore_silence_duration = 2.0\n","  turns.sort_values(by=['start_time'], ascending=True, inplace=True)\n","  n_speakers = len(turns.speaker.unique())\n","\n","  c_turns = pd.DataFrame(columns=[\"speaker\", \"start_time\", \"end_time\", \"turn_type\", \"gap\"])\n","  last_speech = np.empty((n_speakers, 3))\n","  last_speech[:, :] = -1\n","  insert_count = 0\n","  turn_type = \"\"\n","  mutual_silence = 0.0\n","\n","  for index, row in turns.iterrows():\n","    gap = 0.0\n","    other_speakers = [i for i in range(n_speakers) if i != int(row.speaker)]  \n","\n","    if (((~(last_speech[other_speakers, 1] > row.start_time)).all()) and \n","        ((row.start_time - last_speech[int(row.speaker), 1]) < ignore_silence_duration) and \n","        (last_speech[int(row.speaker), 1] > 0)\n","        ): \n","          c_turns.loc[int(last_speech[int(row.speaker), 2]), \"end_time\"] = row.end_time\n","          last_speech[int(row.speaker), 1] = row.end_time\n","\n","    else:\n","        if insert_count == 0:\n","            turn_type = \"LAUNCH\"\n","        elif np.sum([True for x in last_speech[other_speakers] if (row.start_time < x[1] and row.end_time >= x[1]) ]) > 0:\n","            turn_type = \"INTERRUPTION\"\n","        elif np.sum([True for x in last_speech[other_speakers] if (row.start_time < x[1] and row.end_time <= x[1]) ]) > 0:\n","            turn_type = \"OVERLAP\"\n","        elif np.argmax(last_speech[:, 1]) == row.speaker:\n","            turn_type = \"CONTINUE\"\n","            gap = row.start_time - np.max(last_speech[:, 1])\n","            mutual_silence += row.start_time - np.max(last_speech[:, 1])\n","        else:\n","            turn_type = \"RESPONSE\"\n","            gap = row.start_time - np.max(last_speech[:, 1])\n","            if row.start_time - np.max(last_speech[:, 1]) > ignore_silence_duration:\n","                mutual_silence += row.start_time - np.max(last_speech[:, 1])\n","            \n","        c_turns = c_turns.append({\"speaker\": int(row.speaker), \n","                                  \"start_time\": row.start_time, \n","                                  \"end_time\": row.end_time, \n","                                  \"turn_type\": turn_type,\n","                                  \"gap\" : gap}, \n","                                 ignore_index=True)\n","        last_speech[int(row.speaker)] = [row.start_time, row.end_time, insert_count]\n","        insert_count += 1\n","\n","  c_turns[\"duration\"] = c_turns[\"end_time\"] - c_turns[\"start_time\"]\n","\n","  turn_profile = pd.DataFrame(columns=[\"speaker\"])\n","\n","  if n_speakers == 1:\n","    audio_type = \"SOLO\"\n","    mutual_silence = np.nan\n","  else:\n","    audio_type = \"GROUP\"\n","    \n","  turn_profile[\"speaker\"] = [i for i in range(n_speakers)]\n","  turn_profile[\"audio_type\"] = audio_type\n","  turn_profile[\"total_turn_duration\"] = c_turns.groupby(by=\"speaker\").duration.sum()\n","  turn_profile[\"turn_duration\"] = [c_turns[(c_turns.speaker==x)].duration.describe().to_dict() for x in turn_profile.index]\n","  turn_profile[\"speaking_percent\"] = np.divide(turn_profile.total_turn_duration, c_turns.duration.sum())\n","  turn_profile[\"mutual_silence\"] = mutual_silence\n","  turn_profile[\"response_time\"] = [c_turns[(c_turns.speaker==x) & (c_turns.turn_type==\"RESPONSE\")].gap.describe().to_dict() for x in turn_profile.index]\n","  turn_profile[\"interruptions\"] = [c_turns[(c_turns.speaker==x) & (c_turns.turn_type==\"INTERRUPTION\")].duration.describe().to_dict() for x in turn_profile.index]\n","  turn_profile[\"overlap_duration\"] = [c_turns[(c_turns.speaker==x) & (c_turns.turn_type==\"OVERLAP\")].duration.sum() for x in turn_profile.index]\n","  turn_profile[\"overlap\"] = [c_turns[(c_turns.speaker==x) & (c_turns.turn_type==\"OVERLAP\")].duration.describe().to_dict() for x in turn_profile.index]\n","\n","\n","  return c_turns, turn_profile"],"metadata":{"id":"_lv337bjXYV-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_turns(file, c_turns, processor, model, offset=0, samplerate=16000):\n","\n","  \"\"\"\n","  This function consolidates consecutive turns with the same speaker into a single turn on the basis of c_turns and generates the corresponding audio files and transcriptions\n","  \"\"\"\n","\n","  ROOT = os.getcwd()\n","  tmp_dir = os.path.join(ROOT, 'tmp')\n","  os.makedirs(tmp_dir, exist_ok=True)\n","\n","  model.eval()\n","  \n","  for index, row in c_turns.iterrows():\n","    tfm = sox.Transformer()\n","    tfm.trim(row.start_time - offset, row.end_time + offset)\n","    tfm.build_file(f\"input/{file['name']}.wav\", f\"tmp/{file['name']}_turn_{index}.wav\")\n","    audio, sr = librosa.load(f\"tmp/{file['name']}_turn_{index}.wav\", sr=samplerate)\n","\n","    c_turns.loc[index, \"audio_file\"] = f\"tmp/{file['name']}_turn_{index}.wav\"\n","\n","    with torch.no_grad():\n","      input_values = processor(audio, return_tensors=\"pt\", padding=\"longest\", sampling_rate=samplerate).input_values\n","      logits = model(input_values).logits\n","      predicted_ids = torch.argmax(logits, dim=-1)\n","      pred_transcript = processor.batch_decode(predicted_ids)\n","\n","      #torch.cuda.empty_cache() --clear memory if using GPU acceleration\n","      #spellcheck goes here if necessary  \n","\n","      c_turns.loc[index, \"transcript\"] = pred_transcript[0].lower()\n","  \n","  return c_turns"],"metadata":{"id":"QQkhK6ArXnHn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_diarized_transcript(c_turns, display=False):\n","  \n","  \"\"\"\n","  This function takes the c_turns DataFrame and generates a diarized transcript of the conversation, which is saved in the output directory\n","  \"\"\"\n","    \n","  ROOT = os.getcwd()\n","  output_dir = os.path.join(ROOT, 'output')\n","  os.makedirs(output_dir, exist_ok=True)\n","    \n","  with open(f\"output/{DEMO_FILE['name']}_transcript.txt\", \"a\") as file:\n","    for index, row in c_turns.iterrows():\n","      file.writelines(f\"Speaker {row.speaker}: {row.transcript}\\n\") \n","    \n","  if display:\n","    with open(f\"output/{DEMO_FILE['name']}_transcript.txt\", \"r\") as file:\n","      for line in file:\n","        print(line)"],"metadata":{"id":"PTfF9lcfXywn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_speaker_data(c_turns, turn_profile):\n","\n","  \"\"\"\n","  This function extracts speaker-level audio and text data from the list of turns and appends it to the turn_profile DataFrame\n","  \"\"\"\n","\n","  for index, row in turn_profile.iterrows():\n","\n","      turn_profile.loc[index, \"transcript\"] = \" \".join(c_turns[c_turns.speaker == index]['transcript'].to_list())\n","\n","      audio_files = c_turns[c_turns.speaker == index]['audio_file'].to_list()\n","\n","      if len(audio_files) > 1:\n","\n","        cbn =  sox.Combiner()\n","        cbn.build(audio_files, f\"tmp/speaker_{index}.wav\", \"concatenate\")\n","\n","      else:\n","\n","        tfm = sox.Transformer()\n","        tfm.build_file(audio_files[0], f\"tmp/speaker_{index}.wav\")\n","\n","      turn_profile.loc[index, \"audio_file\"] =  f\"tmp/speaker_{index}.wav\"\n","\n","  return turn_profile"],"metadata":{"id":"2tRnJOZtX1rF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def cosine_similarity(a, b):\n","\n","  \"\"\"\n","  This function calculates the cosine similarity between two vectors.\n","  \"\"\"\n","\n","  cos_sim = (a @ b.T) / (norm(a)*norm(b))\n","  return cos_sim"],"metadata":{"id":"GWKSh1XhX44i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calculate_distance(sample_audio, turn_profile, model):\n","\n","  \"\"\"\n","  This function calculates the cosine similarity between the user's sample_audio and each unique speaker in the conversation using the provided model and appends this data to turn_profile\n","  \"\"\"\n","\n","  sample_audio = model(sample_audio)\n","\n","  for index, row in turn_profile.iterrows():\n","\n","    turn_profile.loc[index, 'similarity'] =  cosine_similarity(sample_audio, model(row.audio_file))\n","\n","  return turn_profile"],"metadata":{"id":"UrGAsE1ASbtS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_user_data(turn_profile, threshold):\n","\n","  \"\"\"\n","  This function identifies the speaker from the sample_audio file with one of the speakers in the turn_profile DataFrame within a threshold\"\n","  \"\"\"\n","\n","  if turn_profile['similarity'].max() < threshold:\n","    print(\"User could not be identified. Please manually identify the user.\") #way to log\n","    \n","  else:\n","    speaker_data = turn_profile[turn_profile['similarity'] == turn_profile['similarity'].max()]\n","\n","  return speaker_data"],"metadata":{"id":"zwLt0xl7X8Dc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_filler_word_percent(text, lang_model):\n","\n","  \"\"\"\n","  This function calculates the percentage of filler words in a text using the provided language model\n","  \"\"\"\n","\n","  doc = lang_model(text)\n","  filler_words = [token.text for token in doc if token.pos_ == 'INTJ']\n","  filler_word_pr =  len(filler_words) / len(doc)\n","  \n","  return filler_word_pr"],"metadata":{"id":"ZtOdUR-JX_Il"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_emotions(turn_profile, emotion_model):\n","\n","  \"\"\"\n","  This function assigns a sentiment label to a text using the provided model\n","  \"\"\"\n","\n","  for index, row in turn_profile.iterrows():\n","  \n","    turn_profile.loc[index, 'emotions'] =  emotion_model(row.transcript[:512])[0]['label']\n","\n","  return turn_profile"],"metadata":{"id":"RXzV43Saigms"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_turn_topics(c_turns, turn_profile, topic_tokenizer, topic_model):\n","\n","  \"\"\"\n","  This function assigns topic labels to each turn in c_turns and then appends the number of abrupt topic changes per speaker to turn_profile \n","  \"\"\"\n","\n","  class_mapping = topic_model.config.id2label\n","\n","  for index, row in c_turns.iterrows():\n","    tokens = topic_tokenizer(row.transcript[:514], return_tensors='pt')\n","\n","    output = topic_model(**tokens)\n","\n","    scores = output[0][0].detach().numpy()\n","    scores = expit(scores)\n","    predictions = (scores >= 0.5) * 1\n","\n","    topics = []\n","\n","    for i in range(len(predictions)):\n","      if predictions[i]:\n","        topics.append(class_mapping[i])\n","\n","    c_turns.loc[index, 'topics'] = \",\".join(topics)\n","\n","  responses = c_turns[(c_turns['turn_type'] == 'RESPONSE') | (c_turns['turn_type'] == 'LAUNCH')].reset_index(drop=True)\n","\n","  turn_profile['topic_shifts'] = 0\n","\n","  for index, row in responses.iterrows():\n","\n","    if index == 0:\n","      pass\n","\n","    else:\n","      if set(responses.loc[index - 1, 'topics'].split(\",\")).intersection(set(responses.loc[index, 'topics'].split(\",\"))) == set(): \n","        turn_profile.loc[row.speaker, 'topic_shifts'] += 1\n","\n","  return c_turns, turn_profile"],"metadata":{"id":"sjVUjw85YCAL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#had to remove viterbi=True from crepe for the time beaing\n","\n","def extract_audio_features(audio_file, samplerate):\n","\n","  \"\"\"\n","  This function calculates descriptive statistics for the spectral flatness, volume, and pitch of the provided audio file\n","  \"\"\"\n","\n","  audio, sr = librosa.load(audio_file, sr=samplerate)\n","\n","  flatness = pd.DataFrame(librosa.feature.spectral_flatness(y=audio).T).describe().T\n","  loudness = pd.DataFrame(librosa.feature.rms(audio).T).describe().T\n","  time, frequency, confidence, activation = crepe.predict(audio, sr)\n","  frequency = pd.DataFrame(frequency.T).describe().T\n","\n","  return flatness, loudness, frequency"],"metadata":{"id":"EgjAYIqHYFwx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_features(file, sample_audio, diarization_pipeline, asr_processor, asr_model, emotion_model, topic_tokenizer, topic_model, inference_model, lang_model, offset=0, samplerate=16000, display=False, threshold=.5):\n"," \n","  \"\"\"\n","  This function identifies the user in the provided audio file, extracts their audio, and calculates and returns various features of their speech\n","  \"\"\"\n","  \n","  # denoising goes here\n","\n","  c_turns, turn_profile  = diarization_profiler(file, diarization_pipeline)\n","\n","  c_turns = get_turns(file, c_turns, asr_processor, asr_model, offset, samplerate)\n","\n","  # spellcheck goes here\n","\n","  turn_profile = get_speaker_data(c_turns, turn_profile)\n","\n","  turn_profile = get_emotions(turn_profile, emotion_model)\n","\n","  c_turns, turn_profile = get_turn_topics(c_turns, turn_profile, topic_tokenizer, topic_model)\n","\n","  turn_profile = calculate_distance(sample_audio, turn_profile, inference_model)\n","\n","  speaker_data = get_user_data(turn_profile, threshold).reset_index(drop=True)\n","\n","  flatness, loudness, frequency = extract_audio_features(speaker_data.loc[0, 'audio_file'], samplerate)\n","\n","  features = {}\n","\n","  features['date'] = dt.datetime.now()\n","\n","  features['mutual_silence'] =  speaker_data.loc[0, 'mutual_silence']\n","\n","  features['overlap_duration'] = speaker_data.loc[0, 'overlap_duration']\n","\n","  features['interruptions'] = speaker_data.loc[0, 'interruptions']['count'] \n","\n","  features['total_turn_duration'] = speaker_data.loc[0, 'total_turn_duration']\n","\n","  features['speaking_percent'] = speaker_data.loc[0, 'speaking_percent']\n","\n","  features['response_time'] = speaker_data.loc[0, 'interruptions']['mean']\n","\n","  features['topic_shifts'] = speaker_data.loc[0, 'topic_shifts']\n","\n","  features['emotions'] = speaker_data.loc[0, 'emotions']\n","\n","  features['words_per_minute'] = len(speaker_data.loc[0, 'transcript'].split(\" \")) / (speaker_data.loc[0, 'total_turn_duration'] / 60) \n","\n","  features['fillerword_percent'] = get_filler_word_percent(speaker_data.loc[0, 'transcript'], lang_model)\n","\n","  features['mean_spectral_flatness'] = flatness.loc[0, 'mean'] \n","\n","  features['spectral_flatness_std'] = flatness.loc[0, 'std'] \n","\n","  features['mean_pitch'] = frequency.loc[0, 'mean'] \n","\n","  features['pitch_std'] = frequency.loc[0, 'std'] \n","\n","  features['mean_volume'] = loudness.loc[0, 'mean'] \n","\n","  features['volume_std'] = loudness.loc[0, 'std'] \n","\n","  if speaker_data.loc[0, 'audio_type'] == 'GROUP':\n","    features['is_group'] = 1\n","\n","  else:\n","    features['is_group'] = 0\n","\n","  for file in os.scandir('tmp'):\n","    os.remove(file.path)\n","    \n","  return features"],"metadata":{"id":"1OxJLa0LmpEO","colab":{"base_uri":"https://localhost:8080/","height":152},"executionInfo":{"status":"error","timestamp":1663181003087,"user_tz":300,"elapsed":10,"user":{"displayName":"Aren Wilson-Wright","userId":"05722690186564388330"}},"outputId":"a65a44e4-8902-47b5-a719-d34d001ad6ca"},"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-db7e2691b027>\"\u001b[0;36m, line \u001b[0;32m70\u001b[0m\n\u001b[0;31m    return features\u001b[0m\n\u001b[0m                   \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"]}]},{"cell_type":"code","source":["def generate_lgbm_model(df, skill, random_state=42, n_iter=100):\n","\n","  \"\"\"\n","  This function optimizes a LightGMB regressor model for a particular skill using the provided data. It returns both the model itself and the feature importances of the model for future use.\n","  \"\"\"\n","  \n","  df = df[df['skill'] == skill]\n","  df.drop(columns=['date', 'skill'], inplace=True)\n","  df.fillna(0, inplace=True)\n","  df = pd.get_dummies(df, columns=['emotions'], drop_first=True)\n","  X = df.drop(columns=['score'])\n","  y = df['score']\n","  scaler = StandardScaler()\n","  X_scaled = scaler.fit_transform(X)\n","  X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=.2, random_state=random_state)\n","\n","  space={'n_estimators': hp.quniform('n_estimators', 100, 2000, 1),\n","       'max_depth' : hp.quniform('max_depth', 2, 20, 1),\n","       'learning_rate': hp.loguniform('learning_rate', -5, 0)\n","      }\n","\n","  def lgbm_eval(params, X=X_train, y=y_train, cv=5, random_state=42):\n","    params = {'n_estimators' : int(params['n_estimators']),\n","              'max_depth' : int(params['max_depth']),\n","              'learning_rate' : params['learning_rate']}\n","    lgbm = LGBMRegressor(random_state=random_state, **params)\n","    score = -cross_val_score(lgbm, X, y, cv=cv, scoring='neg_mean_squared_error', n_jobs=-1).mean()\n","    return {\"loss\" : score, \"status\" : STATUS_OK}\n","\n","  trials = Trials()\n","\n","  best = fmin(fn=lgbm_eval, \n","            space=space,\n","            algo=tpe.suggest,\n","            max_evals=n_iter, \n","            trials=trials, \n","            rstate=np.random.RandomState(random_state))\n","\n","  best_model = LGBMRegressor(random_state=random_state, n_estimators=int(best['n_estimators']), max_depth=int(best['max_depth']), learning_rate=best['learning_rate'])\n","\n","  best_model.fit(X_train, y_train)\n","\n","  lgbm_feature_importances = pd.DataFrame({'Feature' : X.columns, 'Importance' : best_model.feature_importances_})\n","\n","  lgbm_feature_importances = lgbm_feature_importances.sort_values(by='Importance', ascending=False).reset_index(drop=True)\n","\n","  return best_model, lgbm_feature_importances"],"metadata":{"id":"lJdnOjT6AvRy"},"execution_count":null,"outputs":[]}]}